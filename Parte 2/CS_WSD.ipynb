{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concept Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "import scipy\n",
    "import re\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from scipy import stats as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wu & Palmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth(s):\n",
    "    return min([len(path) for path in s.hypernym_paths()]) #min dist root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcs(s1, s2): #lowest common synonym\n",
    "    paths1 = s1.hypernym_paths()\n",
    "    paths2 = s2.hypernym_paths()\n",
    "    lcs =  None\n",
    "    min_path = math.inf\n",
    "    for i in range(0, len(paths1)):\n",
    "        for j in range(0, len(paths2)):\n",
    "            n = min(len(paths1[i]), len(paths2[j])) - 1 #min length path (-1 because the first index is 0)\n",
    "            current_path = abs(len(paths1[i]) - len(paths2[j])) # initial distance from lcs\n",
    "            while n >= 0 and current_path < min_path:\n",
    "                if paths1[i][n] == paths2[j][n]: #bottom up search for lcs\n",
    "                    lcs = paths1[i][n]\n",
    "                    min_path = current_path\n",
    "                n -= 1\n",
    "                current_path += 2\n",
    "            return (lcs, min_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wu_palmer(s1, s2):\n",
    "        lcs_pair = lcs(s1, s2)\n",
    "        if lcs_pair[0]:\n",
    "            lcs_depth = depth(lcs_pair[0])\n",
    "            return 2 * lcs_depth / (2 * lcs_depth + lcs_pair[1]) #depth(s1) + depth(s2) = distance(s1, s2) passing by the lcs\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortest Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_depth = max(max(len(hyp_path) for hyp_path in ss.hypernym_paths()) for ss in wn.all_synsets())\n",
    "max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_path(s1, s2):\n",
    "        lcs_pair = lcs(s1, s2)\n",
    "        if lcs_pair[0]:\n",
    "            return 2 * max_depth - lcs_pair[1]\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leakcock & Chodorow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leakcock_chodorow(s1, s2):\n",
    "        lcs_pair = lcs(s1, s2)\n",
    "        if lcs_pair[0]:\n",
    "            return - math.log((lcs_pair[1] + 1)/(2 * max_depth + 1), 10)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open(\"res\\WordSim353.csv\", \"r\") as file:\n",
    "    for i in file.readlines():\n",
    "            lines.append(i.strip().split(','))\n",
    "#print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson WP:  0.3133778521262506\n",
      "Pearson SP:  0.18660311573550914\n",
      "Pearson LC:  0.3274173550208445\n",
      "Spearman WP:  0.3450816969196533\n",
      "Spearman SP:  0.300305123168255\n",
      "Spearman LC:  0.300305123168255\n"
     ]
    }
   ],
   "source": [
    "res_wp = []\n",
    "res_sp = []\n",
    "res_lc = []\n",
    "human_res = []\n",
    "for i in range(1, len(lines) - 1):\n",
    "    human_res.append(float(lines[i][2]))\n",
    "    set1 = wn.synsets(lines[i][0])\n",
    "    set2 = wn.synsets(lines[i][1])\n",
    "    maximum_wp = 0\n",
    "    maximum_sp = 0\n",
    "    maximum_lc = 0\n",
    "    for syn1 in set1:\n",
    "        for syn2 in set2:\n",
    "            wp = wu_palmer(syn1, syn2)\n",
    "            maximum_wp = max(maximum_wp, wp)\n",
    "            \n",
    "            sp = shortest_path(syn1, syn2)\n",
    "            maximum_sp = max(maximum_sp, sp)\n",
    "            \n",
    "            lc = leakcock_chodorow(syn1, syn2)\n",
    "            maximum_lc = max(maximum_lc, lc)\n",
    "            \n",
    "    res_wp.append(maximum_wp)\n",
    "    res_sp.append(maximum_sp)\n",
    "    res_lc.append(maximum_lc)\n",
    "\n",
    "print(\"Pearson WP: \", st.pearsonr(res_wp, human_res)[0])\n",
    "print(\"Pearson SP: \", st.pearsonr(res_sp, human_res)[0])\n",
    "print(\"Pearson LC: \", st.pearsonr(res_lc, human_res)[0])\n",
    "print(\"Spearman WP: \", st.spearmanr(res_wp, human_res)[0])\n",
    "print(\"Spearman SP: \", st.spearmanr(res_sp, human_res)[0])\n",
    "print(\"Spearman LC: \", st.spearmanr(res_lc, human_res)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesk Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk(word, sentence): #bag of words approach at WD\n",
    "    best_sense = []\n",
    "    max_overlap = -1\n",
    "    context = set(sentence.split())\n",
    "    for s in wn.synsets(word):\n",
    "        signature = set(s.definition().split())\n",
    "        for e in s.examples():\n",
    "            signature.update(e.split())\n",
    "        overlap = len(context & signature)\n",
    "        if overlap > max_overlap:\n",
    "            best_sense = s\n",
    "            max_overlap = overlap\n",
    "    return best_sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "sentences = []\n",
    "with open(\"res\\sentences.txt\", \"r\") as file:\n",
    "    for i, line in enumerate(file):\n",
    "        if i >= 3 and i < 17:\n",
    "            sentence = line.strip()[2:-1]\n",
    "            word = re.findall(r\"\\*\\*\\w+\\*\\*\", sentence)[0]\n",
    "            words.append(word[2:-2])\n",
    "            sentences.append(sentence)\n",
    "#print(words)\n",
    "#print(sentences)\n",
    "#print(lesk(words[1], sentences[1]).lemmas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arm'] bend at the elbow\n",
      "Germany sells ['arm'] to Saudi Arabia\n",
      "The ['key'] broke in the lock\n",
      "The ['cardinal', 'central', 'fundamental', 'key', 'primal'] problem was not one of quality but of quantity\n",
      "Work out the ['solution', 'answer', 'result', 'resolution', 'solvent'] in your head\n",
      "Heat the ['solution', 'answer', 'result', 'resolution', 'solvent'] to 75Ã‚Â° Celsius\n",
      "The house was burnt to ['ash'] while the owner returned\n",
      "This table is made of ['ash'] wood\n",
      "The ['lunch', 'luncheon', 'tiffin', 'dejeuner'] with her boss took longer than she expected\n",
      "She packed her ['lunch'] in her purse\n",
      "The ['categorization', 'categorisation', 'classification', 'compartmentalization', 'compartmentalisation', 'assortment'] of the genetic data took two years\n",
      "The journal Science published the ['categorization', 'categorisation', 'classification', 'compartmentalization', 'compartmentalisation', 'assortment'] this month\n",
      "His cottage is near a small ['forest', 'wood', 'woods']\n",
      "The statue was made out of a block of ['wood']\n"
     ]
    }
   ],
   "source": [
    "for pair in zip(words, sentences):\n",
    "    print(pair[1].replace(\"**\" + pair[0] + \"**\", str([l.name() for l in lesk(pair[0], pair[1]).lemmas()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SemCor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import semcor as sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is:  50.90909090909091 %\n"
     ]
    }
   ],
   "source": [
    "tagged_sentences = [[str(c) for c in s] for s in sc.tagged_sents(tag='both')[:50]] #get first 50 sentences as sting lists\n",
    "sentences = sc.sents()\n",
    "correct_answers = 0\n",
    "answers = 0\n",
    "for i, s in enumerate(tagged_sentences):\n",
    "    sentence = \" \".join(sentences[i])\n",
    "    for w in s:  \n",
    "        if \"Lemma\" in w and \"NN \" in w:\n",
    "            w = w.split(\" (NN \")\n",
    "            lemma = w[0][1:]\n",
    "            word = w[1][:-2]\n",
    "            p_lemma = lesk(word.replace(\" \", \"_\"), sentence)\n",
    "            if p_lemma and lemma in map(str, p_lemma.lemmas()):\n",
    "                correct_answers += 1\n",
    "            answers += 1\n",
    "print(\"accuracy is: \", correct_answers/answers * 100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
